{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML for Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the CV parts are on the appendix. Helper functions are on helper.py. All the missing code is in the appendix/helper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the Data to Decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Decided to focus on 3 decisions, each get different piece of the data, saving the class balance:<br>\n",
    " Feature Engineering - 70% of train data<br>\n",
    " Feature Selection - 15% of train data<br>\n",
    " Model Tuning - will use Catboost - 15% of train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will make a feature matrix from 4 data sets: food_train, food_nutrients, nutrients,snacks images.<br>\n",
    "We use cross validation for features extracted from the following fields:<br>\n",
    "- household fulltext<br>\n",
    "- description<br>\n",
    "- ingredients<br>\n",
    "- brand<br>\n",
    "\n",
    "For each these we split into 4 folds and examine meaningful extractable features.<br>\n",
    "\n",
    "For other sources of data (e.g image, nutrients), we settle for the work done in the exploratory phase, or in one \"plain vanilla\" option.<br>\n",
    "\n",
    "After these stages, we merge all resulting features and apply feature selection (see below)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Food Nutrients Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add all nutrients as features.<br>\n",
    "Beacuse each decision is taken in additive way we start with nutrients and only then check other features. <br>\n",
    "We don't need to scale the nutrients becuase we will use Tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  # nutrients in snack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look on #nutrients per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_train['# nutrients'] = food_train[[col for col in food_train.columns if 'nutrient_' in col]].count(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "cakes_cupcakes_snack_cakes              14.428518\n",
       "candy                                   11.848182\n",
       "chips_pretzels_snacks                   15.248447\n",
       "chocolate                               13.709958\n",
       "cookies_biscuits                        14.591782\n",
       "popcorn_peanuts_seeds_related_snacks    14.788303\n",
       "Name: # nutrients, dtype: float64"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_train_fe.groupby('category')['# nutrients'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add #nutrients as a feature beacuse it seems to have information about the category separation - this feature gets different value by average for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nutrient_amount(data):\n",
    "    data['# nutrients'] = data[[col for col in data.columns if 'nutrient_' in col]].count(axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### # of nutrients per unit for each snack - Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Household full text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will use a data devision that allocated for houehold column, first we will focus on cleaning this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1451,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_decision_df = food_train_fe[fold_indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Unique values of household_serving_fulltext: 971\n"
     ]
    }
   ],
   "source": [
    "print(\"#Unique values of household_serving_fulltext:\",len(houshold_decision_df.household_serving_fulltext.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1417,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_decision_df = clean_column(household_decision_df,'household_serving_fulltext') # clean_column on helper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look on the values we get after cleaning the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unique values of household_serving_fulltext after cleaning:  227\n"
     ]
    }
   ],
   "source": [
    "print('# Unique values of household_serving_fulltext after cleaning: ',len(houshold_decision_df.household_serving_fulltext.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see 'a cake' when so we will remove 'stop words' of english - words as 'the','a' etc...<br>\n",
    "We also see that we have pop and pops, pack and packs, cake and cakes and etc..<br>\n",
    "Therfore we will use stemmimg that will keep us only the root of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_stop_words and stemmimg functions are in helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1420,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_decision_df['household_serving_fulltext'] = household_decision_df.household_serving_fulltext.map(lambda a: remove_stop_words(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1424,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_decision_df['household_serving_fulltext'] = household_decision_df.household_serving_fulltext.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unique values of household_serving_fulltext remove stop words:  204\n"
     ]
    }
   ],
   "source": [
    "print('# Unique values of household_serving_fulltext remove stop words: ',len(household_decision_df.household_serving_fulltext.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1427,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_decision_df['household_serving_fulltext'] = household_decision_df.household_serving_fulltext.map(lambda a: stemming(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1428,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_decision_df = clean_column(household_decision_df,'household_serving_fulltext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unique values of household_serving_fulltext after cleaning:  171\n"
     ]
    }
   ],
   "source": [
    "print('# Unique values of household_serving_fulltext after cleaning: ',len(household_decision_df.household_serving_fulltext.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will satisfy for now with this level of cleaning, from 971 unique values to 171."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the household column, we want to decrease the number of levels (not by cleaning each string).<br>\n",
    "\n",
    "We will check if its better to use the option below:<br>\n",
    "We will count for each category the n top values of houshold column, and those will be our keywords.<br>\n",
    "If a the houshold column contains one of the keywords -> we will change the column to that word.<br>\n",
    "It's possible to check different size of n, we will choose 10,20,50,70,90.<br>\n",
    "We are using top n words from each category so the class imbalance won't neglect any class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will do the same for n = [10,20,50,70,90]\n",
    "houshold_n_list = [10,20,50,70,90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "metadata": {},
   "outputs": [],
   "source": [
    "houshold_keywords_dict_top_n = {n: set(sum({category: get_column_top_n_word_count_by_category('household_serving_fulltext',category,houshold_decision_df,n) for category in categories}.values(), [])) for n in houshold_n_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding those columns to the df\n",
    "for n in houshold_n_list:\n",
    "    household_decision_df[f'houshold_manual_top_{n}'] = household_decision_df.household_serving_fulltext.map(lambda a: replace_to_keyword(houshold_keywords_dict_top_n[n],a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the best n with CV - Code in the appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1122,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_houshold_mean_kf_score = {key: np.array(kf_fe_dict_houshold[key]).mean() for key in kf_fe_dict_houshold.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: 0.6246121194853298,\n",
       " 20: 0.6296513284421966,\n",
       " 50: 0.619214331505093,\n",
       " 70: 0.6183144033827843,\n",
       " 90: 0.6183144033827843,\n",
       " 'household_serving_fulltext': 0.6300113644457971}"
      ]
     },
     "execution_count": 1123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe_houshold_mean_kf_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Decided to leave household_serving_fulltext as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### houshold unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was shown in the exploratory, a known measures in household unit might be a good feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_is_houshold_unit(data):\n",
    "    units_keywords = ['onz','cup', 'grm','tbsp','pouch','ounce','tsp']\n",
    "    is_houshold_unit = data.household_serving_fulltext.map(lambda a: is_keywords_in(units_keywords,a))\n",
    "    data['is_houshold_unit'] = is_houshold_unit\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1433,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_decision_df = add_is_houshold_unit(household_decision_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will use a data devision that allocated for description column, first we will focus on cleaning this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1128,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_decision_df = food_train_fe[fold_indices[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1129,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_decision_df = process(description_decision_df,'description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's check if there is a difference in the #words between the different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_decision_df['# description words'] = description_decision_df.description.str.count(' ') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "cakes_cupcakes_snack_cakes              3.789474\n",
       "candy                                   4.019417\n",
       "chips_pretzels_snacks                   4.460815\n",
       "chocolate                               4.634494\n",
       "cookies_biscuits                        4.289700\n",
       "popcorn_peanuts_seeds_related_snacks    4.033309\n",
       "Name: # description words, dtype: float64"
      ]
     },
     "execution_count": 1131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_decision_df['# description words'] = description_decision_df.description.str.count(' ') + 1\n",
    "description_decision_df[['description','# description words']].sort_values(by='# description words', ascending=True)\n",
    "description_decision_df.groupby('category')['# description words'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not a big difference but it could be a feature..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Words in descriptions:  23124\n",
      "# Unique Words in descriptions:  2725\n"
     ]
    }
   ],
   "source": [
    "full_description_word_list = get_full_description_word_list(description_decision_df) #helper.py\n",
    "print('# Words in descriptions: ',len(full_description_word_list))\n",
    "print('# Unique Words in descriptions: ',len(set(full_description_word_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check 2 options to extract features from description:\n",
    "1. We will count for each category the top n used words values of description column, and those will be our keywords.\n",
    "    The features will be if a the description column contain one of the keywords.\n",
    "    we will check different size of n.\n",
    "2. using word2vec vectors as features - will check different size of vocabulary by filtering the number of appearences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual word count by category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to make a boolean feature if a description contains a keyword for each of the keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions on appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =  10\n",
      "#description_keywords 46\n",
      "n =  20\n",
      "#description_keywords 87\n",
      "n =  50\n",
      "#description_keywords 192\n",
      "n =  70\n",
      "#description_keywords 249\n",
      "n =  90\n",
      "#description_keywords 311\n",
      "n =  110\n",
      "#description_keywords 369\n"
     ]
    }
   ],
   "source": [
    "description_n_list = [10,20,50,70,90,110]\n",
    "for n in description_n_list:\n",
    "    description_decision_df = add_features_from_column(description_decision_df,'description',n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use word2vec library and check different sizes of vocabulary to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_list = [10,20,25,30,35,40,45]\n",
    "# Getting the filtered list for minimum apperences of 5,10,20\n",
    "full_description_filterd_dict = {n: get_filtered_list(full_description_word_list,n) for n in n_list}\n",
    "# Remove the words in the df that don't appear on that list.\n",
    "description_for_vec_dict = {n: description_decision_df.description.map(lambda a: remove_keywords_from_df_column(a,full_description_filterd_dict[n],\" \")) for n in n_list}\n",
    "# Getting Word2Vec model for each n\n",
    "model_description_word2vec_dict = {n:  Word2Vec(description_for_vec_dict[n].to_list(), vector_size=40, window=15, min_count=5, workers=4,epochs=100) for n in n_list}\n",
    "# Getting words of Word2Vec model for each n\n",
    "words_description_word2vec_dict = {n: list(model_description_word2vec_dict[n].wv.index_to_key) for n in n_list}\n",
    "# Getting words of Word2Vec model for each n\n",
    "vectors_description_word2vec_dict = {n: np.asarray(model_description_word2vec_dict[n].wv.vectors) for n in n_list}\n",
    "# Create a dictionary of mapping word to vector for each n  \n",
    "description_word2vec_mapping_dict = {n: {words_description_word2vec_dict[n][i]: vectors_description_word2vec_dict[n][i] for i in range(len(words_description_word2vec_dict[n]))} for n in n_list}\n",
    "#Map the Word2Vec vectors to each ingredient and calculate the average of each coordinate in the vectors of each sentence. \n",
    "description_vectors_dict= {n: description_for_vec_dict[n].map(lambda a: calc_mean_word2vec(a,description_word2vec_mapping_dict[n])) for n in n_list}\n",
    "# Eventually add each averaged coordinate as a feature.\n",
    "for n in n_list:\n",
    "    df = pd.DataFrame(description_vectors_dict[n])\n",
    "    description_decision_df[[f'description_{n}_filtered_vec_{i}' for i in range(0,40)]]  = pd.DataFrame(df.description.tolist(), columns=['description_'+str(i) for i in range(0,40)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check word2vec VS. manual word count with different size of n with CV - Code in the Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'manual10': 0.8855522602619974,\n",
       " 'manual20': 0.8889709834292783,\n",
       " 'manual50': 0.8878920086253231,\n",
       " 'manual70': 0.8862715228357368,\n",
       " 'manual90': 0.8868117387278296,\n",
       " 'manual110': 0.8880717028537386,\n",
       " 'description': 0.8527991827959773,\n",
       " 'word2vec10': 0.853878967033394,\n",
       " 'word2vec20': 0.8547788951557026,\n",
       " 'word2vec25': 0.855498157729442,\n",
       " 'word2vec30': 0.8549587512708106,\n",
       " 'word2vec35': 0.8547785713823182,\n",
       " 'word2vec40': 0.8553186253877186,\n",
       " 'word2vec45': 0.8544185353787178}"
      ]
     },
     "execution_count": 1445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf_fe_dict_description_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description as it is got the lowest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'manual20'"
      ]
     },
     "execution_count": 1447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(kf_fe_dict_description_mean.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual 20 won!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will use a data devision that allocated for ingredients column, first we will focus on cleaning this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1144,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_decision_df = food_train_fe[fold_indices[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After moving hence and forth with the ingredients cleaning, we decided to:<br>\n",
    "    - remove all text indise all brackets<br>\n",
    "    - take the string that comes after \"of:\", \"following:\", \"icing:\", \"including:\", \"less:\", \"ingredients:\"<br>\n",
    "    - group flour values that seems to be wheat flour<br>\n",
    "    - more technical decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran word2vec on the ingredients and used kmeans for clustering on the vectors and got this as similiar group (without 'unbleach flour','bleach flour').<br>\n",
    "So it strenghened our intuition from the exploratory to combine those words as 'flour'.(see the group on appendix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_ingredients and process_ingredients are at the helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1146,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_decision_df = procces_ingredients(ingredients_decision_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Ingredients in all snacks:  58087\n",
      "# Unique Ingredients in all snacks:  5715\n"
     ]
    }
   ],
   "source": [
    "full_ingredients = get_full_ingredients_list(ingredients_decision_df)\n",
    "print('#Ingredients in all snacks: ',len(full_ingredients))\n",
    "print('# Unique Ingredients in all snacks: ',len(set(full_ingredients)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for Ingredients we will extract:<br> \n",
    "1. All ingredients embedded with word2vec- will check different size of vocabulary by filtering number of appearences.<br>\n",
    "2.We will count for each category the top n used words values of ingredients column, and those will be our keywords.<br>\n",
    "    The features will be if a the houshold ingredients contain one of the keywords.<br>\n",
    "    we will check different size of n.<br>\n",
    "3. The first ingredient as a feature<br>\n",
    "We will check by CV what is the best feature to extract.<br>\n",
    "In addition we will add the number of ingredients as a feature as seen in the exploratory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same with n = 10,20,50\n",
    "n_list = [10,20,50,70,90]\n",
    "for n in n_list:\n",
    "    ingredients_decision_df = add_features_from_column(ingredients_decision_df,'ingredients',n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Ingredient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2        corn starch,eggs,vegetable oil,leavening,natur...\n",
       "12       sugar,flour,soybean oil,eggs,water,milk,modifi...\n",
       "16       flour,whole wheat graham flour,cane sugar,vege...\n",
       "20                               ginger,sugar,citric acid,\n",
       "24       blend root vegetable,expeller pressed canoloil...\n",
       "                               ...                        \n",
       "30312    corn syrup,high oleic canoloil,soybean oil,bar...\n",
       "30317                         potatoes,vegetable oil,salt,\n",
       "30364                         potatoes,vegetable oil,salt,\n",
       "30401               roasted chickpea,wasabi soy seasoning,\n",
       "30439               native andean potatoe,palm oil,sesalt,\n",
       "Name: ingredients, Length: 5557, dtype: object"
      ]
     },
     "execution_count": 881,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients_decision_df.ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_decision_df['first_ingredient'] = ingredients_decision_df.ingredients.str.split(',',n=1,expand=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique first ingredient values in the dataset:  943\n"
     ]
    }
   ],
   "source": [
    "print('unique first ingredient values in the dataset: ',len(set(ingredients_decision_df['first_ingredient'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top First Ingredient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It could be a good feature (see appendix) but probably has to many levels. We will examine few n top values to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_first_ingredient_list = [50,100,150,200,250]\n",
    "for n in n_first_ingredient_list:\n",
    "    top_values = get_top_values_by_categories(n,'first_ingredient',ingredients_decision_df)\n",
    "    ingredients_decision_df[f'first_ingredient_{n}'] = ingredients_decision_df.first_ingredient.map(lambda a: 'other' if a not in top_values else a) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec on all ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we filter all ingredients by minimum amount of appereances.<br>\n",
    "We will perform Word2Vec for minimum apperences of 5,10,20 and check by cross validation what gives the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_word2vec(column,word2vec_mapping_dict):  \n",
    "    if column != []:\n",
    "        result = [word2vec_mapping_dict[i].astype('float') for i in column]\n",
    "        result =  np.mean(result,axis = 0 ).astype('float')\n",
    "        return result\n",
    "    return [np.nan for i in range(0,40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_list = [5,10,20,25,30,35,40,45,50]\n",
    "# Getting the filtered list for minimum apperences of 5,10,20\n",
    "full_ingredients_filterd_dict = {n: get_filtered_list(full_ingredients,n) for n in n_list}\n",
    "# Remove the words in the df that don't appear on that list.\n",
    "ingredients_for_vec_dict = {n: ingredients_decision_df.ingredients.map(lambda a: remove_keywords_from_df_column(a,full_ingredients_filterd_dict[n],\",\")) for n in n_list}\n",
    "# Getting Word2Vec model for each n\n",
    "model_ingredients_word2vec_dict = {n:  Word2Vec(ingredients_for_vec_dict[n].to_list(), vector_size=40, window=15, min_count=5, workers=4,epochs=100) for n in n_list}\n",
    "# Getting words of Word2Vec model for each n\n",
    "words_ingredients_word2vec_dict = {n: list(model_ingredients_word2vec_dict[n].wv.index_to_key) for n in n_list}\n",
    "# Getting words of Word2Vec model for each n\n",
    "vectors_ingredients_word2vec_dict = {n: np.asarray(model_ingredients_word2vec_dict[n].wv.vectors) for n in n_list}\n",
    "# Create a dictionary of mapping word to vector for each n  \n",
    "ingredients_word2vec_mapping_dict = {n: {words_ingredients_word2vec_dict[n][i]: vectors_ingredients_word2vec_dict[n][i] for i in range(len(words_ingredients_word2vec_dict[n]))} for n in n_list}\n",
    "#Map the Word2Vec vectors to each ingredient and calculate the average of each coordinate in the vectors of each sentence. \n",
    "ingredients_vectors_dict= {n: ingredients_for_vec_dict[n].map(lambda a: calc_mean_word2vec(a,ingredients_word2vec_mapping_dict[n])) for n in n_list}\n",
    "# Eventually add each averaged coordinate as a feature.\n",
    "for n in n_list:\n",
    "    df = pd.DataFrame(ingredients_vectors_dict[n])\n",
    "    ingredients_decision_df[[f'ingredients_{n}_filtered_vec_{i}' for i in range(0,40)]]  = pd.DataFrame(df.ingredients.tolist(), columns=['ingredients_'+str(i) for i in range(0,40)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CV between ingredients options - code in the appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_ingredients_mean_kf_score = {key: np.array(kf_fe_dict_ingredients[key]).mean() for key in kf_fe_dict_ingredients.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'manual10': 0.8599913228732946,\n",
       " 'manual20': 0.8594509450945095,\n",
       " 'manual50': 0.8711480680442145,\n",
       " 'manual70': 0.8702484636952905,\n",
       " 'manual90': 0.871148391817599,\n",
       " 'word2vec5': 0.857832078171846,\n",
       " 'word2vec10': 0.8571116823912608,\n",
       " 'word2vec20': 0.8581921141754464,\n",
       " 'word2vec25': 0.8581922760621387,\n",
       " 'word2vec30': 0.858191952288754,\n",
       " 'word2vec35': 0.8571128155981066,\n",
       " 'word2vec40': 0.8576518982833535,\n",
       " 'word2vec45': 0.8571126537114143,\n",
       " 'word2vec50': 0.8587320062941546,\n",
       " 'first_ingredient_50': 0.8718681400514153,\n",
       " 'first_ingredient_100': 0.8729472767420626,\n",
       " 'first_ingredient_150': 0.8718674925046461,\n",
       " 'first_ingredient_200': 0.8738480142978327,\n",
       " 'first_ingredient_250': 0.8720483199399076}"
      ]
     },
     "execution_count": 1162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe_ingredients_mean_kf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first_ingredient_200'"
      ]
     },
     "execution_count": 1163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(fe_ingredients_mean_kf_score.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Ingredient with 200 top levels is chosen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_decision_df['# ingredients'] = ingredients_decision_df.ingredients.str.count(',') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "cakes_cupcakes_snack_cakes              22.611360\n",
       "candy                                   11.420659\n",
       "chips_pretzels_snacks                    9.390578\n",
       "chocolate                                9.889236\n",
       "cookies_biscuits                        13.334812\n",
       "popcorn_peanuts_seeds_related_snacks     6.448557\n",
       "Name: # ingredients, dtype: float64"
      ]
     },
     "execution_count": 896,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients_decision_df.groupby('category')['# ingredients'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It could be a good feature, we can see clearly difference in # ingredients between the categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will use a data devision that allocated for brand column, first we will focus on cleaning this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_decision_df = food_train_fe[fold_indices[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1179,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_decision_df = clean_brand(brand_decision_df)# helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unique Brands: 1900\n"
     ]
    }
   ],
   "source": [
    "print(\"# Unique Brands:\", len(brand_decision_df.brand.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will examine 2 options to extract featurs out of brand:<br>\n",
    "\n",
    "1. we will change all the brands that are not in the top n of each category, all other brands will go to 'other' category to decrease the number of levels.<br>\n",
    "we will check different size of n.<br>\n",
    "2. We will produce keywords by top n words appearences from each category.<br>\n",
    "The motivation for that is repeating words as chocolate, bakery and candy inside the brand names.<br>\n",
    "we will check different size of n.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_values_by_categories(n,column,data):\n",
    "    print(\"n = \", n)\n",
    "    result = []\n",
    "    for category in categories:\n",
    "        result.extend(data[column][data.category == category].value_counts().index.get_level_values(0).to_list()[0:n])\n",
    "    result = list(set(result))\n",
    "    print(\"# values\", len(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_brand_list = [50,100,150,200,250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 100, 150, 200, 250]"
      ]
     },
     "execution_count": 951,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_brand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_brand_list = [50,100,150,200,250]\n",
    "for n in n_brand_list:\n",
    "    top_values = get_top_values_by_categories(n,'brand',brand_decision_df)\n",
    "    brand_decision_df[f'brand_top_{n}'] = brand_decision_df.brand.map(lambda a: 'other' if a not in top_values else a) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see words as candy and chocolate on the brand names the might imply the snack.\n",
    "In order to do word count we have to clean better the brand column - with stemming and remove stop words\n",
    "We will use our 'process' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1181,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_decision_df = process(brand_decision_df,'brand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_top_n_list = [10,20,50,70,90,110]\n",
    "for n in brand_top_n_list:\n",
    "    food_train_fe = add_features_from_column(brand_decision_df,'brand',n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check top n brands VS. top n word count in CV - code in the appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_brand_mean_kf_score = {key: np.array(kf_fe_dict_brand[key]).mean() for key in kf_fe_dict_brand.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brand_top_50': 0.8673701183067948,\n",
       " 'brand_top_100': 0.8715095610280452,\n",
       " 'brand_top_150': 0.8709691832492602,\n",
       " 'brand_top_200': 0.8697090572366589,\n",
       " 'brand_top_250': 0.8718678162780307,\n",
       " 'word_count_top_10': 0.8652108736053462,\n",
       " 'word_count_top_20': 0.8675504600819794,\n",
       " 'word_count_top_50': 0.8659311074992392,\n",
       " 'word_count_top_70': 0.8657501181772854,\n",
       " 'word_count_top_90': 0.8675501363085949,\n",
       " 'word_count_top_110': 0.8668303880747785}"
      ]
     },
     "execution_count": 1188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe_brand_mean_kf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brand_top_250'"
      ]
     },
     "execution_count": 1189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(fe_brand_mean_kf_score.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 250 values from each category of brand is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grm Serving Size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create serving_size column by grm, we will multiply the serving size with serving size unit of ml by 1000. <br>\n",
    "There is no need to scale the data becuase we use tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to extract features from the given images of snacks (per category), we chose to train an independent CNN model on the portion of the train set dedicated for feature engineering (this portion was selected to preserve the class proportion in the entire set).<br> The stand alone model's last layer contains a 128-sized vector that is the input to the last layer's softmax decision function.<br> This layer represents important information contained in the image that is useful for the required classification.<br> We will use this representation as additional features in the top-level model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use keras ImageDataGenerator feature to load the data used for training.<br> We use a default network architecture shown below, with 0.001 learning rate, 128 batch size and an RMSProp optimizer.<br>\n",
    " All these are model hyperparameters that might be optimized. However we restrict ourselves to one option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "pic_dim = 140\n",
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        fe_train_dir,  \n",
    "        target_size=(pic_dim, pic_dim),  \n",
    "        batch_size=batch_size,        \n",
    "        classes = categories,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(pic_dim, pic_dim, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The third convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fourth convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fifth convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a dense layer\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 128 neuron in the fully-connected layer\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    # 5 output neurons for 5 classes with the softmax activation\n",
    "    tf.keras.layers.Dense(6, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "total_sample=train_generator.n\n",
    "n_epochs = 30\n",
    "import scipy\n",
    "print (scipy.__version__)\n",
    "if scipy is None:\n",
    "    print('no scipy')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(learning_rate=0.001),\n",
    "              metrics=['acc'])\n",
    "history = model.fit(\n",
    "        train_generator, \n",
    "        steps_per_epoch=int(total_sample/batch_size),  \n",
    "        epochs=n_epochs,\n",
    "        verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting model is then cropped using keras's pop method, leaving a model whose output is a 128-sized vector.<br> We then use this new model to extract features on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras import layers\n",
    "\n",
    "model.pop()\n",
    "model.summary()\n",
    "# Model(inputs=model.inputs, outputs=model.layers[-1].output).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the images are black and white, hence we use keras augmentation functions to stnadardaize the input to the best possible manner (make sure all images are converted to valid dimension tensors).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is_good_image function on appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have 270 features and we would like to examine their importance.<br> We will perform 5 fold CV on a new training set dedicated for this decision with the features we created on the feture engineering part.<br> We will use the feature importance of catbooat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1210,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_train_fs =  food_train.loc[food_train['idx'].isin(feature_selection_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4764, 272)"
      ]
     },
     "execution_count": 1211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_train_fs.shape # contain category and idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Averaged accuracy we got over the 5 folds of the data for feature selection:(code in the appendix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5619503205269515"
      ]
     },
     "execution_count": 1215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([report_dict[i]['accuracy'] for i in range(0,5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.18782791185729275,\n",
       " 0.8048268625393494,\n",
       " 0.7565582371458552,\n",
       " 0.3672612801678909,\n",
       " 0.6932773109243697]"
      ]
     },
     "execution_count": 1216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[report_dict[i]['accuracy'] for i in range(0,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Features: 270\n"
     ]
    }
   ],
   "source": [
    "print(\"# Features:\", len(columns_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got pretty bad accuracy, probably had overfitting in the feature engineering decisions.<br>\n",
    "Maybe it has to do with the order of our additive decisions also that kind of canceling here - we are not assuming to have any feature in advanced and we choose the best features, but in feature engineering part, on each decision we assumed to have the features we added before.<br>\n",
    "\n",
    "We will choose n features out of all the features by counting how many times each feature appered on the top 100 features the folds.<br> we will choose features that appeared in at least 3 folds.\n",
    "We know 100 is chosen without any approval, but we want to avoid of making few decisions on the same data set, so we take this guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1218,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_fe_dicts = [dict(sorted(report_dict[i]['feature_importance'].items(), key=lambda item: item[1])) for i in range(0,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1219,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists_of_most_important_features_per_fold = [list(sorted_fe_dicts[i].keys())[0:100] for i in range(0,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1220,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = [l for f in lists_of_most_important_features_per_fold  for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "fe_count = Counter(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1230,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = [i[0] for i in fe_count.most_common(100) if i[1] > 3] #no more than 100 features that appear in at least 4 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_features_beyoned_2 = [i[0] for i in fe_count.most_common(100) if i[1] > 2]\n",
    "# pd.DataFrame(top_features_beyoned_2).to_csv('top_features_beyoned_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 1231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature list seems to be ok, let's see the result on the model tuning part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this part we will apply grid search on the catboost parameters. We will use GridSearchCV and choost the best parameters on new training set, with the features selected on feature selection part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1234,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_columns = top_features + ['category','idx']\n",
    "food_train_mt = food_train_mt[[col for col in food_train_mt.columns if col in mt_columns ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 400 are chosen! (code in the appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the Full Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to apply the features we created and chose on the test set and train our model on all the train set. (loading images model that was trained on the entire train set in the appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Features Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected features will be features who appeared on the top 100 features on 3 folds and more on the feature selection part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 1513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top features model on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1492,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_class_train_set = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4269786778369185"
      ]
     },
     "execution_count": 1495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(food_train['category'], preds_class_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not the best result, let's see what happens if we are using all the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All features model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 1512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(columns_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All features model  predictions on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9474032313942868"
      ]
     },
     "execution_count": 1510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(food_train['category'], preds_class_train_set_all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result on the test set is much better but it might be becuase of overfitting...\n",
    "We can't know which of the models will perform better on the test set, so those models predictions will be our submissions.\n",
    "We will try one more model - with top features with lower threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top features with lower threshold model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected features will be features who appeared on the top 100 features on 2 folds and more on the feature selection part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 1511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_features_beyoned_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5415262511416963"
      ]
     },
     "execution_count": 1515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(food_train['category'], preds_class_train_set_top_features_beyoned_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is better than top features, again we can't know which model will perform best on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
